# Video Captioning

Human labeling of videos is expensive and time-consuming. 
We adopt powerful image captioning models to generate 
captions for videos. HPC-AI captioned their training videos 
with the [PLLaVA](https://github.com/magic-research/PLLaVA) model. 
PLLaVA performs highly competitively on multiple 
video-based text generation benchmarks including 
[MVbench](https://paperswithcode.com/sota/video-question-answering-on-mvbench?p=pllava-parameter-free-llava-extension-from-1).

## PLLaVA Captioning

First, download the model on HuggingFace:

| Model      | Initialized From                                                                                              |
| ------------ | --------------------------------------------------------------------------------------------------------------- |
| pllava-7b  | [llava-hf/llava-v1.6-vicuna-7b-hf · Hugging Face](https://huggingface.co/llava-hf/llava-v1.6-vicuna-7b-hf)   |
| pllava-13b | [llava-hf/llava-v1.6-vicuna-13b-hf · Hugging Face](https://huggingface.co/llava-hf/llava-v1.6-vicuna-13b-hf) |
| pllava-34b | [llava-hf/llava-v1.6-34b-hf · Hugging Face](https://huggingface.co/llava-hf/llava-v1.6-34b-hf)               |

And put it under `./pretrained_models/pllava`. You can 
also use a customized directory by using the option
`--pretrained_model_name_or_path` when running the script.

Currently, we only support captioning on Ascend.

```bash
export PYTHONPATH=$(pwd)
msrun --worker_num=2 --local_worker_num=2 --join=True \
 --log_dir=msrun_log pipeline/captioning/caption_pllava.py \
 /path/to/meta.csv 
```
Modify `worker_num` and `local_worker_num` based on your resource.

We support the following arguments for PLLaVA captioning:

- `pretrained_model_name_or_path`: the PLLaVA model directory.
- `pooling shape`: Default (16, 12, 12).
- `num_frames`: the number of frames to extract from the videos. Default 4.
- `use_lora`: whether to use LORA. `weight_dir` must be provided. **(Remark: Not yet tested)**
- `weight_dir`: Weight directory for LORA.
- `question`: The prompt used to generate captions. Default "Describe the video in details".
- `max_new_tokens`: Maximum new tokens generated by PLLaVA, default 200.
- `num_beams`: Number of beams for search. Default 1.
- `temperature`: Controls output randomness. Default 1.0.
- `pad_length`: The maximum padding length for each Tensor to ensure all Tensors are of equal lengths before `AllGather`. Default 4096.
- `bs`: Batch size. Default 1.
- `skip_if_existing`: Skip processing if output already exists. Default False.

**NOTE:** When running large-scale parallel inference, 
the default `HCCL_CONNECT_TIMEOUT` setting might be 
insufficient, potentially causing runtime errors with 
AllGather operations. To avoid this issue, consider 
setting `export HCCL_CONNECT_TIMEOUT=7200` (corresponds to 
7200 seconds) or adjusting it according to your 
specific needs.


